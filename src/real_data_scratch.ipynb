{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "worthy-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from src.models import *\n",
    "from src.loss_functions import *\n",
    "from src.noise import *\n",
    "from src.metrics import *\n",
    "from src.plotting import *\n",
    "from src.generate_data import *\n",
    "from src.real_data import *\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "from operator import xor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "muslim-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_noise_and_train_model(m, max_iter, X_train, y_train, X_test, y_test, p_y_x_dict, noise_type = \"class_independent\", uncertainty_type=\"backward\",  model_type = \"LR\" , fixed_class=0, fixed_noise=0.2, T_true = None, T_est = None, batch_size = 512, base_seed = 2024, epsilon = 0.25):\n",
    "    \n",
    "    if uncertainty_type == \"forward\":\n",
    "        loss_types = [\"Ours\", \"BCE\", \"backward\", \"forward\"]\n",
    "        y_vec = y_train\n",
    "    else: # backward\n",
    "        loss_types = [\"Ours\"]\n",
    "\n",
    "        #Initial Noise Draw\n",
    "        u_vec = get_u(y_train, T = T_true, seed= base_seed, noise_type = noise_type)\n",
    "        y_vec = flip_labels(y_train, u_vec) #XOR\n",
    "\n",
    "    metrics = MetricsStorage(loss_types)\n",
    "\n",
    "    preds_train_dict = {loss: [] for loss in loss_types}\n",
    "    preds_test_dict = {loss: [] for loss in loss_types}\n",
    "\n",
    "    typical_count = 0\n",
    "\n",
    "    \n",
    "    for seed in tqdm(range(1, max_iter+1)):\n",
    "        if uncertainty_type == \"forward\":\n",
    "            # Using a forward model, so get u directly\n",
    "            u_vec = get_u(y_vec, T = T_true, seed= seed, noise_type = noise_type)\n",
    "        else:\n",
    "            u_vec = infer_u(y_vec, noise_type = noise_type, p_y_x_dict = p_y_x_dict,  T = T_est , seed=seed)\n",
    "\n",
    "        typical_flag, difference = is_typical(u_vec, p_y_x_dict,  T = T_est, y_vec = y_vec, noise_type = noise_type, uncertainty_type = uncertainty_type, epsilon = epsilon)\n",
    "\n",
    "        if not typical_flag: \n",
    "            continue\n",
    "\n",
    "        flipped_labels = flip_labels(y_vec, u_vec)\n",
    "\n",
    "        if uncertainty_type == \"forward\":\n",
    "            for loss in loss_types:\n",
    "                if loss == \"Ours\":\n",
    "                    model,  (train_acc,\n",
    "                        test_acc,\n",
    "                        train_probs,\n",
    "                        test_probs,\n",
    "                        train_loss,\n",
    "                        test_loss,\n",
    "                        train_preds,\n",
    "                        test_preds\n",
    "                        ) = train_model_ours(X_train, flipped_labels, X_test, y_test, seed = 2024, model_type=model_type)\n",
    "\n",
    "                    preds_train_dict[loss].append(train_preds)\n",
    "                    preds_test_dict[loss].append(test_preds)\n",
    "\n",
    "                    metrics.add_metric(loss, \"noisy_train_loss\", train_loss)\n",
    "                    metrics.add_metric(loss, \"noisy_train_acc\", train_acc*100)\n",
    "                    metrics.add_metric(loss, \"clean_test_loss\", test_loss)\n",
    "                    metrics.add_metric(loss, \"clean_test_acc\", test_acc*100)\n",
    "                    metrics.add_metric(loss, \"typical_difference\", difference)\n",
    "                    metrics.add_metric(loss, \"preds_train\", train_preds)\n",
    "                    metrics.add_metric(loss, \"preds_test\", test_preds)\n",
    "                    metrics.add_metric(loss, \"train_probs\", train_probs)\n",
    "                    metrics.add_metric(loss, \"test_probs\", test_probs)\n",
    "\n",
    "                else:\n",
    "                    model,  (noisy_train_loss,\n",
    "                            clean_train_loss, \n",
    "                            noisy_train_acc,\n",
    "                            clean_train_acc,\n",
    "                            train_probs,\n",
    "                            clean_test_loss, \n",
    "                            clean_test_acc,\n",
    "                            test_probs\n",
    "                            ) = train_model(X_train, y_train, flipped_labels,  X_test, y_test,  T = T_est, seed=2024, num_epochs=25, batch_size=batch_size, model_type = model_type, correction_type=loss)\n",
    "\n",
    "                    preds_train = (train_probs > 0.5).astype(int)\n",
    "                    preds_test = (test_probs > 0.5).astype(int)\n",
    "\n",
    "                    preds_train_dict[loss].append(preds_train)\n",
    "                    preds_test_dict[loss].append(preds_test)\n",
    "\n",
    "                    metrics.add_metric(loss, \"noisy_train_loss\", noisy_train_loss)\n",
    "                    metrics.add_metric(loss, \"clean_train_loss\", clean_train_loss)\n",
    "                    metrics.add_metric(loss, \"noisy_train_acc\", noisy_train_acc*100)\n",
    "                    metrics.add_metric(loss, \"clean_train_acc\", clean_train_acc*100)\n",
    "                    metrics.add_metric(loss, \"clean_test_loss\", clean_test_loss)\n",
    "                    metrics.add_metric(loss, \"clean_test_acc\", clean_test_acc*100)\n",
    "                    metrics.add_metric(loss, \"flip_frequency\", sum(u_vec)/len(u_vec))\n",
    "                    metrics.add_metric(loss, \"typical_difference\", difference)\n",
    "                    metrics.add_metric(loss, \"preds_train\", preds_train)\n",
    "                    metrics.add_metric(loss, \"preds_test\", preds_test)\n",
    "                    metrics.add_metric(loss, \"train_probs\", train_probs)\n",
    "                    metrics.add_metric(loss, \"test_probs\", test_probs)\n",
    "\n",
    "        else: #backward_sk\n",
    "            \n",
    "            for loss in loss_types:\n",
    "                model,  (train_acc,\n",
    "                        test_acc,\n",
    "                        train_probs,\n",
    "                        test_probs,\n",
    "                        train_loss,\n",
    "                        test_loss,\n",
    "                        train_preds,\n",
    "                        test_preds\n",
    "                        ) = train_model_ours(X_train, flipped_labels, X_test, y_test, seed = 2024, model_type=model_type)\n",
    "\n",
    "                preds_train_dict[loss].append(train_preds)\n",
    "                preds_test_dict[loss].append(test_preds)\n",
    "\n",
    "                metrics.add_metric(loss, \"train_loss\", train_loss)\n",
    "                metrics.add_metric(loss, \"train_acc\", train_acc*100)\n",
    "                metrics.add_metric(loss, \"test_loss\", test_loss)\n",
    "                metrics.add_metric(loss, \"test_acc\", test_acc*100)\n",
    "                metrics.add_metric(loss, \"typical_difference\", difference)\n",
    "                metrics.add_metric(loss, \"preds_train\", train_preds)\n",
    "                metrics.add_metric(loss, \"preds_test\", test_preds)\n",
    "                metrics.add_metric(loss, \"train_probs\", train_probs)\n",
    "                metrics.add_metric(loss, \"test_probs\", test_probs)\n",
    "\n",
    "        typical_count += 1\n",
    "\n",
    "        if typical_count == m:\n",
    "            break\n",
    "\n",
    "    for loss in loss_types:\n",
    "        typical_rate = typical_count / seed\n",
    "        print(\"Typical Rate: \", typical_rate)\n",
    "        metrics.add_metric(loss, \"typical_rate\", typical_rate)\n",
    "\n",
    "        predictions_train = np.array(preds_train_dict[loss])\n",
    "\n",
    "        predictions_test = np.array(preds_test_dict[loss])\n",
    "\n",
    "        try:\n",
    "            regret_train = calculate_error_rate(predictions_train, y_train)\n",
    "            disagreement_train = estimate_disagreement(predictions_train)\n",
    "\n",
    "            regret_test = calculate_error_rate(predictions_test, y_test)\n",
    "            disagreement_test = estimate_disagreement(predictions_test)\n",
    "\n",
    "        except:\n",
    "            print(\"Error: Could not get Disagreement Metrics\")\n",
    "            continue\n",
    "\n",
    "        for i, item in enumerate(X_train):\n",
    "            metrics.add_metric(loss, \"regret_train\", regret_train[i])\n",
    "            metrics.add_metric(loss, \"disagreement_train\", disagreement_train[i])\n",
    "\n",
    "        for i, item in enumerate(X_test):\n",
    "\n",
    "            metrics.add_metric(loss, \"regret_test\", regret_test[i])\n",
    "            metrics.add_metric(loss, \"disagreement_test\", disagreement_test[i])\n",
    "\n",
    "    print(\"DONE\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, yn_train, X_test, y_test, T,  seed, num_epochs=25, batch_size = 512, correction_type=\"forward\", model_type = \"LR\"):\n",
    "    # Check if GPU is available and set the default device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Convert to PyTorch tensors and move them to the device\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    yn_train = torch.tensor(yn_train, dtype=torch.long).to(device)\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # Create DataLoader for mini-batch SGD\n",
    "    train_data = TensorDataset(X_train, yn_train, y_train)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"LR\":\n",
    "        # Initialize the model and move it to the device\n",
    "        model = LogisticRegression(X_train.shape[1]).to(device)\n",
    "    else:\n",
    "        # Initialize the model and move it to the device\n",
    "        model = NeuralNet(X_train.shape[1]).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    if correction_type in ['backward', 'forward']:\n",
    "        #elapsed = timeit.default_timer() - start_time\n",
    "        #T = torch.tensor(np.array([T_dict[g.item()] for g in group]), dtype=torch.float32).to(device)\n",
    "        #print(\"T_dict \", elapsed)\n",
    "\n",
    "        T = torch.tensor(T).to(device)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in (range(num_epochs)):\n",
    "        for features, noisy_labels, clean_labels in train_loader:\n",
    "            \n",
    "\n",
    "            # Move features and labels to the device\n",
    "            features, noisy_labels, clean_labels = features.to(device), noisy_labels.to(device), clean_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "\n",
    "            if correction_type == 'forward':\n",
    "                noisy_loss = forward_loss(outputs, noisy_labels, T, device)\n",
    "\n",
    "                #elapsed = timeit.default_timer() - start_time\n",
    "                #print(\"Forward \", elapsed)\n",
    "\n",
    "            elif correction_type == 'backward':\n",
    "                noisy_loss = backward_loss(outputs, noisy_labels, T, device)\n",
    "\n",
    "                #elapsed = timeit.default_timer() - start_time\n",
    "                #print(\"Backward \", elapsed)\n",
    "\n",
    "            else:\n",
    "                noisy_loss = criterion(outputs, noisy_labels)\n",
    "                \n",
    "                #elapsed = timeit.default_timer() - start_time\n",
    "                #print(\"BCE \", elapsed)\n",
    "            #print(noisy_loss.item())\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            noisy_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    train_outputs = model(X_train)\n",
    "    test_outputs = model(X_test)\n",
    "\n",
    "    #Get final train losses\n",
    "    if correction_type == 'forward':\n",
    "        \n",
    "        #T_train = torch.tensor(np.array([T_dict[g.item()] for g in group_train]), dtype=torch.float32).to(device)\n",
    "        #T_test = torch.tensor(np.array([T_dict[g.item()] for g in group_test]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        noisy_train_loss = forward_loss(train_outputs, yn_train, T, device).item()\n",
    "        clean_train_loss = forward_loss(train_outputs, y_train, T, device).item()\n",
    "        \n",
    "        clean_test_loss = forward_loss(test_outputs, y_test, T, device).item()\n",
    "        \n",
    "    elif correction_type == 'backward':\n",
    "        #T_train = torch.tensor(np.array([T_dict[g.item()] for g in group_train]), dtype=torch.float32).to(device)\n",
    "        #T_test = torch.tensor(np.array([T_dict[g.item()] for g in group_test]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        noisy_train_loss = backward_loss(train_outputs, yn_train, T, device).item()\n",
    "        clean_train_loss = backward_loss(train_outputs, y_train, T, device).item()\n",
    "        \n",
    "        clean_test_loss = backward_loss(test_outputs, y_test, T, device).item()\n",
    "    else:\n",
    "        noisy_train_loss = criterion(train_outputs, yn_train).item()\n",
    "\n",
    "        clean_train_loss = criterion(train_outputs, y_train).item()\n",
    "        clean_test_loss = criterion(test_outputs, y_test).item()\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        # Move the predictions back to the CPU for sklearn accuracy calculation\n",
    "        clean_test_acc = accuracy_score(y_test.cpu().numpy(), predicted.cpu().numpy())\n",
    "        test_probs = torch.softmax(test_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "        _, predicted = torch.max(train_outputs.data, 1)\n",
    "        # Move the predictions back to the CPU for sklearn accuracy calculation\n",
    "        clean_train_acc = accuracy_score(y_train.cpu().numpy(), predicted.cpu().numpy())\n",
    "        noisy_train_acc = accuracy_score(yn_train.cpu().numpy(), predicted.cpu().numpy())\n",
    "        train_probs = torch.softmax(train_outputs, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    results = (noisy_train_loss,\n",
    "                clean_train_loss, \n",
    "                noisy_train_acc,\n",
    "                clean_train_acc,\n",
    "                train_probs,\n",
    "                clean_test_loss, \n",
    "                clean_test_acc,\n",
    "                test_probs\n",
    "                )\n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "silver-pressure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12203, 52)\n",
      "(3051,)\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cshock_mimic\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = load_dataset_splits(dataset, group = \"age\")\n",
    "\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "royal-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_type = \"class_independent\"\n",
    "\n",
    "fixed_class = 1\n",
    "fixed_noise = 0.0\n",
    "\n",
    "model_type = \"NN\"\n",
    "uncertainty_type = \"forward\"\n",
    "if dataset == \"cshock_eicu\":\n",
    "    batch_size = 512\n",
    "elif dataset == \"lungcancer\":\n",
    "    batch_size = 2048\n",
    "else:\n",
    "    batch_size = 1024\n",
    "\n",
    "max_iter = 10000\n",
    "m = 10\n",
    "d= X_train.shape[1]\n",
    "noise_level = 0.2\n",
    "\n",
    "\n",
    "misspecify = False\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "p_y_x_dict =  calculate_prior(y_train, noise_type = noise_type, group=group_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "oriented-trunk",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, T_true = generate_class_independent_noise(y_train, noise_level) #Fixed noise draw\n",
    "\n",
    "\n",
    "if misspecify == True: #Misspecified T\n",
    "    pass #TODO\n",
    "    T_est = None\n",
    "else: #Correct T\n",
    "    T_est = T_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "geological-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d36c8d83d94fd28c15d40b6f39f330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typical Rate:  1.0\n",
      "Typical Rate:  1.0\n",
      "Typical Rate:  1.0\n",
      "Typical Rate:  1.0\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "metrics = simulate_noise_and_train_model(m, max_iter,  X_train, y_train, X_test, y_test, p_y_x_dict, T_true = T_true, T_est = T_est, noise_type = noise_type, model_type = model_type, uncertainty_type=uncertainty_type,  fixed_class=fixed_class, fixed_noise=fixed_noise, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "representative-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.79318256309406"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metrics.data[\"BCE\"][\"clean_test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "pediatric-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.21632251720749"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metrics.data[\"Ours\"][\"clean_test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "incoming-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.56440511307767"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metrics.data[\"backward\"][\"clean_test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "republican-appointment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.14716486397904"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metrics.data[\"forward\"][\"clean_test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "moving-costume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.281650845706463"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metrics.data[\"backward\"][\"noisy_train_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-connection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-baghdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noisyTS",
   "language": "python",
   "name": "noisyts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
